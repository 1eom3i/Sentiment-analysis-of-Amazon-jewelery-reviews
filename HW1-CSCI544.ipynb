{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4 \n",
    "#in case you don't have it installed\n",
    "# Dataset: https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Jewelry_v1_00.tsv.gz\n",
    "! pip install -U gensim\n",
    "import nltk\n",
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int):\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "import contractions\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.linear_model import Perceptron, LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 10437: expected 15 fields, saw 22\\nSkipping line 10443: expected 15 fields, saw 22\\nSkipping line 19872: expected 15 fields, saw 22\\nSkipping line 20055: expected 15 fields, saw 22\\nSkipping line 20107: expected 15 fields, saw 22\\nSkipping line 20167: expected 15 fields, saw 22\\nSkipping line 53858: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 72173: expected 15 fields, saw 22\\nSkipping line 84308: expected 15 fields, saw 22\\nSkipping line 92156: expected 15 fields, saw 22\\nSkipping line 97791: expected 15 fields, saw 22\\nSkipping line 106812: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 138899: expected 15 fields, saw 22\\nSkipping line 145840: expected 15 fields, saw 22\\nSkipping line 192385: expected 15 fields, saw 22\\nSkipping line 194126: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 222872: expected 15 fields, saw 22\\nSkipping line 236587: expected 15 fields, saw 22\\nSkipping line 245017: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 273895: expected 15 fields, saw 22\\nSkipping line 302200: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 390306: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 395443: expected 15 fields, saw 22\\nSkipping line 399564: expected 15 fields, saw 22\\nSkipping line 435836: expected 15 fields, saw 22\\nSkipping line 436589: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 477174: expected 15 fields, saw 22\\nSkipping line 502547: expected 15 fields, saw 22\\nSkipping line 514925: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 533989: expected 15 fields, saw 22\\nSkipping line 534712: expected 15 fields, saw 22\\nSkipping line 545993: expected 15 fields, saw 22\\nSkipping line 574954: expected 15 fields, saw 22\\nSkipping line 576486: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 604707: expected 15 fields, saw 22\\nSkipping line 613292: expected 15 fields, saw 22\\nSkipping line 622491: expected 15 fields, saw 22\\nSkipping line 648193: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 686930: expected 15 fields, saw 22\\nSkipping line 691337: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 728126: expected 15 fields, saw 22\\nSkipping line 728320: expected 15 fields, saw 22\\nSkipping line 746281: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 831631: expected 15 fields, saw 22\\nSkipping line 849295: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 888781: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 962366: expected 15 fields, saw 22\\nSkipping line 976631: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 983531: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1070597: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1191390: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1258986: expected 15 fields, saw 22\\nSkipping line 1260478: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1318010: expected 15 fields, saw 22\\nSkipping line 1375879: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1570444: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1690885: expected 15 fields, saw 22\\nSkipping line 1699783: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1756053: expected 15 fields, saw 22\\n'\n",
      "C:\\Users\\leo\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3058: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    " df=pd.read_csv(\"amazon_reviews_us_Jewelry_v1_00.tsv\",sep='\\t',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "star_rating\n",
       "1     155000\n",
       "2     100791\n",
       "3     159644\n",
       "4     270420\n",
       "5    1080847\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df[['star_rating']]=df[['star_rating']].astype(int)\n",
    "df['review_body']=df[['review_headline', 'review_body']].apply(\" \".join, axis=1)\n",
    "df_select=df[['star_rating','review_body']]\n",
    "df.groupby(['star_rating']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Dataset Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1199322</td>\n",
       "      <td>1</td>\n",
       "      <td>Not a lip ring! To small to be a lip ring! So ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>858715</td>\n",
       "      <td>1</td>\n",
       "      <td>Design has chain attached to bangle with dangl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215960</td>\n",
       "      <td>1</td>\n",
       "      <td>two small for my 9 years old and very difficul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1611668</td>\n",
       "      <td>1</td>\n",
       "      <td>These earrings WERE NOT a pleasant surprise bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22545</td>\n",
       "      <td>1</td>\n",
       "      <td>Huge!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225058</td>\n",
       "      <td>5</td>\n",
       "      <td>Top quality. Great ring. It looks just as good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1744151</td>\n",
       "      <td>5</td>\n",
       "      <td>this is a very pretty ring, I wear it as a wed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>693646</td>\n",
       "      <td>5</td>\n",
       "      <td>Beautiful bracelet my daughter lives it thank you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>856725</td>\n",
       "      <td>5</td>\n",
       "      <td>This bracelet is very pretty and a wonderful w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1711390</td>\n",
       "      <td>5</td>\n",
       "      <td>Bought the chain for a pendant I purchased as ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "1199322            1  Not a lip ring! To small to be a lip ring! So ...\n",
       "858715             1  Design has chain attached to bangle with dangl...\n",
       "215960             1  two small for my 9 years old and very difficul...\n",
       "1611668            1  These earrings WERE NOT a pleasant surprise bu...\n",
       "22545              1                                              Huge!\n",
       "...              ...                                                ...\n",
       "225058             5  Top quality. Great ring. It looks just as good...\n",
       "1744151            5  this is a very pretty ring, I wear it as a wed...\n",
       "693646             5  Beautiful bracelet my daughter lives it thank you\n",
       "856725             5  This bracelet is very pretty and a wonderful w...\n",
       "1711390            5  Bought the chain for a pendant I purchased as ...\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a balanced dataset of 100k reviews\n",
    "def prepare(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df[['star_rating']]=df[['star_rating']].astype(int)\n",
    "    df_select=df[['star_rating','review_body']]\n",
    "\n",
    "    gp=shuffle(df_select[df_select.star_rating==1])\n",
    "    gp=gp.head(20000)\n",
    "    temp=gp\n",
    "    for i in [2,3,4,5]:\n",
    "        gp=shuffle(df_select[df_select.star_rating==i])\n",
    "        gp=gp.head(20000)\n",
    "        temp=pd.concat([temp,gp])\n",
    "        \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Pretrained Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_g=api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of excellent and outstanding:0.55674857\n"
     ]
    }
   ],
   "source": [
    "print('similarity of excellent and outstanding:'+str(model_g.similarity('excellent','outstanding')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King-Man+Woman=\n",
      "[('queen', 0.7118193507194519)]\n"
     ]
    }
   ],
   "source": [
    "print('King-Man+Woman=')\n",
    "print(model_g.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Train my own Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1749548</td>\n",
       "      <td>1</td>\n",
       "      <td>Do not order this belly ring! What you see in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1036966</td>\n",
       "      <td>1</td>\n",
       "      <td>Photo is very deceptive The photo is not the r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1172066</td>\n",
       "      <td>1</td>\n",
       "      <td>LOOKED LIKE A GUMBALL TRINKET(but not as nice)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1749746</td>\n",
       "      <td>1</td>\n",
       "      <td>live,  love and laugh bracelet Engraving was b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332978</td>\n",
       "      <td>1</td>\n",
       "      <td>One Star Does not look nice at all in person, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>837340</td>\n",
       "      <td>5</td>\n",
       "      <td>Love it This necklace took a while to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1661508</td>\n",
       "      <td>5</td>\n",
       "      <td>Great. I was shopping for nice, shiny blue top...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1553782</td>\n",
       "      <td>5</td>\n",
       "      <td>Like it! I bought this for my friend as a pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339543</td>\n",
       "      <td>5</td>\n",
       "      <td>Five Stars Perfect size and very pretty ,  My ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>851559</td>\n",
       "      <td>5</td>\n",
       "      <td>big but beautiful A little big for my taste bu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "1749548            1  Do not order this belly ring! What you see in ...\n",
       "1036966            1  Photo is very deceptive The photo is not the r...\n",
       "1172066            1  LOOKED LIKE A GUMBALL TRINKET(but not as nice)...\n",
       "1749746            1  live,  love and laugh bracelet Engraving was b...\n",
       "332978             1  One Star Does not look nice at all in person, ...\n",
       "...              ...                                                ...\n",
       "837340             5  Love it This necklace took a while to receive ...\n",
       "1661508            5  Great. I was shopping for nice, shiny blue top...\n",
       "1553782            5  Like it! I bought this for my friend as a pres...\n",
       "339543             5  Five Stars Perfect size and very pretty ,  My ...\n",
       "851559             5  big but beautiful A little big for my taste bu...\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a balanced dataset of 100k reviews\n",
    "def prepare(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df[['star_rating']]=df[['star_rating']].astype(int)\n",
    "    df_select=df[['star_rating','review_body']]\n",
    "\n",
    "    gp=shuffle(df_select[df_select.star_rating==1])\n",
    "    gp=gp.head(20000)\n",
    "    temp=gp\n",
    "    for i in [2,3,4,5]:\n",
    "        gp=shuffle(df_select[df_select.star_rating==i])\n",
    "        gp=gp.head(20000)\n",
    "        temp=pd.concat([temp,gp])\n",
    "        \n",
    "    return temp\n",
    "\n",
    "dataset=prepare(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749548    [do, not, order, this, belly, ring, what, you,...\n",
       "1036966    [photo, is, very, deceptive, the, photo, is, n...\n",
       "1172066    [looked, like, a, gumball, trinket, but, not, ...\n",
       "1749746    [live, love, and, laugh, bracelet, engraving, ...\n",
       "332978     [one, star, does, not, look, nice, at, all, in...\n",
       "                                 ...                        \n",
       "837340     [love, it, this, necklace, took, a, while, to,...\n",
       "1661508    [great, i, was, shopping, for, nice, shiny, bl...\n",
       "1553782    [like, it, i, bought, this, for, my, friend, a...\n",
       "339543     [five, stars, perfect, size, and, very, pretty...\n",
       "851559     [big, but, beautiful, a, little, big, for, my,...\n",
       "Name: review_body, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=dataset\n",
    "#lowercase\n",
    "temp['review_body']=temp.review_body.str.lower()\n",
    "#perform contractions\n",
    "temp['review_body']=temp['review_body'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "temp['review_body']=[' '.join(map(str, l)) for l in temp['review_body']]\n",
    "#remove tag\n",
    "temp['review_body'] = temp['review_body'].apply(lambda cw: BeautifulSoup(cw).get_text())\n",
    "temp['review_body'] = temp['review_body'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "#tokenize\n",
    "temp['review_body']=temp['review_body'].apply(nltk.word_tokenize)\n",
    "# remove non-alphabetical\n",
    "temp['review_body'] = temp['review_body'].apply(lambda x: [re.sub('[^a-zA-Z]', '', word) for word in x])\n",
    "temp['review_body']=[' '.join(map(str, l)) for l in temp['review_body']]\n",
    "#tokenize\n",
    "temp['review_body']=temp['review_body'].apply(nltk.word_tokenize)\n",
    "temp['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dataset = temp['review_body'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model=gensim.models.Word2Vec(vec_dataset, vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "King-Man+Woman=\n",
      "[('differ', 0.542309045791626)]\n"
     ]
    }
   ],
   "source": [
    "print('King-Man+Woman=')\n",
    "print(my_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity of excellent and outstanding:0.7342691\n"
     ]
    }
   ],
   "source": [
    "print('similarity of excellent and outstanding:'+str(my_model.wv.similarity('excellent','outstanding')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Explanation:\n",
    "In case of 'king+woman-man=queen', the pretrained model performs better. In case of 'excellent~outstanding', my model performs better. The reason behind the result might be our dataset is reviews of jewellery and therefore excellent, outstanding will be two words that appear more frequently which let our model predict the similarity better. However, queen, king may appear less in our dataset than in the google dataset, and therefore pretrained model predicts this similarity better. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>star_rating</th>\n",
       "      <th>review_body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1445861</td>\n",
       "      <td>1</td>\n",
       "      <td>Ehh, okay ring I choose this product for a New...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1549772</td>\n",
       "      <td>1</td>\n",
       "      <td>Gold or Plastic ??? This was to be a special g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1174944</td>\n",
       "      <td>1</td>\n",
       "      <td>Cheap POS Ordered this necklace as a gift and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>921182</td>\n",
       "      <td>1</td>\n",
       "      <td>very disappointed very disappointed in product...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1479498</td>\n",
       "      <td>1</td>\n",
       "      <td>Piece of junk my husband got this for me for C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>638727</td>\n",
       "      <td>5</td>\n",
       "      <td>Original necklace. Love it. Very beautiful. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1756357</td>\n",
       "      <td>5</td>\n",
       "      <td>Love it! This Ring Holder is so much more fun ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1745468</td>\n",
       "      <td>5</td>\n",
       "      <td>Silver Hoop Earrings Good quality, good size e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1189472</td>\n",
       "      <td>5</td>\n",
       "      <td>Beautiful with a beautiful verse Beautiful, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186363</td>\n",
       "      <td>5</td>\n",
       "      <td>I have the necklace and bracelet and love them...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         star_rating                                        review_body\n",
       "1445861            1  Ehh, okay ring I choose this product for a New...\n",
       "1549772            1  Gold or Plastic ??? This was to be a special g...\n",
       "1174944            1  Cheap POS Ordered this necklace as a gift and ...\n",
       "921182             1  very disappointed very disappointed in product...\n",
       "1479498            1  Piece of junk my husband got this for me for C...\n",
       "...              ...                                                ...\n",
       "638727             5  Original necklace. Love it. Very beautiful. I ...\n",
       "1756357            5  Love it! This Ring Holder is so much more fun ...\n",
       "1745468            5  Silver Hoop Earrings Good quality, good size e...\n",
       "1189472            5  Beautiful with a beautiful verse Beautiful, an...\n",
       "186363             5  I have the necklace and bracelet and love them...\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build a balanced dataset of 100k reviews\n",
    "def prepare(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df[['star_rating']]=df[['star_rating']].astype(int)\n",
    "    df_select=df[['star_rating','review_body']]\n",
    "\n",
    "    gp=shuffle(df_select[df_select.star_rating==1])\n",
    "    gp=gp.head(20000)\n",
    "    temp=gp\n",
    "    for i in [2,3,4,5]:\n",
    "        gp=shuffle(df_select[df_select.star_rating==i])\n",
    "        gp=gp.head(20000)\n",
    "        temp=pd.concat([temp,gp])\n",
    "        \n",
    "    return temp\n",
    "\n",
    "dataset=prepare(df)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445861    [ehh, okay, ring, i, choose, this, product, fo...\n",
       "1549772    [gold, or, plastic, this, was, to, be, a, spec...\n",
       "1174944    [cheap, pos, ordered, this, necklace, as, a, g...\n",
       "921182     [very, disappointed, very, disappointed, in, p...\n",
       "1479498    [piece, of, junk, my, husband, got, this, for,...\n",
       "                                 ...                        \n",
       "638727     [original, necklace, love, it, very, beautiful...\n",
       "1756357    [love, it, this, ring, holder, is, so, much, m...\n",
       "1745468    [silver, hoop, earrings, good, quality, good, ...\n",
       "1189472    [beautiful, with, a, beautiful, verse, beautif...\n",
       "186363     [i, have, the, necklace, and, bracelet, and, l...\n",
       "Name: review_body, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp=dataset\n",
    "#lowercase\n",
    "temp['review_body']=temp.review_body.str.lower()\n",
    "#perform contractions\n",
    "temp['review_body']=temp['review_body'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "temp['review_body']=[' '.join(map(str, l)) for l in temp['review_body']]\n",
    "#remove tag\n",
    "temp['review_body'] = temp['review_body'].apply(lambda cw: BeautifulSoup(cw).get_text())\n",
    "temp['review_body'] = temp['review_body'].apply(lambda x: re.sub(r\"http\\S+\", \"\", x))\n",
    "#tokenize\n",
    "temp['review_body']=temp['review_body'].apply(nltk.word_tokenize)\n",
    "# remove non-alphabetical\n",
    "temp['review_body'] = temp['review_body'].apply(lambda x: [re.sub('[^a-zA-Z]', '', word) for word in x])\n",
    "temp['review_body']=[' '.join(map(str, l)) for l in temp['review_body']]\n",
    "#tokenize\n",
    "temp['review_body']=temp['review_body'].apply(nltk.word_tokenize)\n",
    "temp['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1445861    [ehh, okay, ring, choose, product, new, year, ...\n",
       "1549772    [gold, plastic, special, gift, arrived, honest...\n",
       "1174944    [cheap, po, ordered, necklace, gift, totally, ...\n",
       "921182     [disappointed, disappointed, product, son, bou...\n",
       "1479498    [piece, junk, husband, got, christmas, let, te...\n",
       "                                 ...                        \n",
       "638727     [original, necklace, love, beautiful, get, ton...\n",
       "1756357    [love, ring, holder, much, fun, stuffy, crysta...\n",
       "1745468    [silver, hoop, earring, good, quality, good, s...\n",
       "1189472    [beautiful, beautiful, verse, beautiful, beaut...\n",
       "186363     [necklace, bracelet, love, earring, necklace, ...\n",
       "Name: review_body, Length: 100000, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stop words\n",
    "stop_words=set(stopwords.words('english'))\n",
    "temp['review_body']=temp['review_body'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "#perform lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "temp['review_body'] = temp['review_body'].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])\n",
    "temp['review_body']=[' '.join(map(str, l)) for l in temp['review_body']]\n",
    "\n",
    "temp['review_body']=temp['review_body'].apply(nltk.word_tokenize)\n",
    "temp['review_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset=temp['review_body'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dataset=[' '.join(map(str, l)) for l in temp['review_body']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDF feature extraction\n",
    "tfIdfVectorizer=TfidfVectorizer(use_idf=True, max_features=5000)\n",
    "tfIdf = tfIdfVectorizer.fit_transform(tfidf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels\n",
    "y=temp['star_rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(tfIdf, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Word2Vec feature extraction\n",
    "w2v_ls_simple=np.zeros((100000,300),dtype='float32')\n",
    "for i in range(len(my_dataset)):\n",
    "    vec=[]\n",
    "    for j in my_dataset[i]:\n",
    "        if j in model_g.key_to_index:\n",
    "            xxx=model_g[j].astype('float32')\n",
    "            vec.append(xxx)\n",
    "        \n",
    "    if not vec:\n",
    "        vec=[np.zeros(model_g[my_dataset[0][0]].shape, dtype='float32')]\n",
    "    a=np.mean(vec, dtype='float32', axis=0)\n",
    "    w2v_ls_simple[i]=a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2_train, X2_test, y2_train, y2_test = train_test_split(w2v_ls_simple, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Word2Vec perceptron:0.4798\n",
      "Accuracy of TFIdf perceptron:0.55045\n"
     ]
    }
   ],
   "source": [
    "#Perceptron\n",
    "w2v_p = Perceptron(tol=1e-2, random_state=1)\n",
    "w2v_p.fit(X2_train, y2_train)\n",
    "print('Accuracy of Word2Vec perceptron:'+ str(w2v_p.score(X2_test, y2_test)))\n",
    "\n",
    "tf_p = Perceptron(tol=1e-2, random_state=1)\n",
    "tf_p.fit(X1_train, y1_train)\n",
    "print('Accuracy of TFIdf perceptron:'+ str(tf_p.score(X1_test, y1_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Word2Vec SVM:0.5447\n",
      "Accuracy of TFIdf SVM:0.6065\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "w2v_svm = LinearSVC()\n",
    "w2v_svm.fit(X2_train, y2_train)\n",
    "w2v_svm.score(X2_test,y2_test)\n",
    "print('Accuracy of Word2Vec SVM:'+ str(w2v_svm.score(X2_test, y2_test)))\n",
    "\n",
    "tf_svm = LinearSVC()\n",
    "tf_svm.fit(X1_train, y1_train)\n",
    "tf_svm.score(X1_test,y1_test)\n",
    "print('Accuracy of TFIdf SVM:'+ str(tf_svm.score(X1_test, y1_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Explanation: In both cases of SVM and Perceptron, the TF-IDF features have a better accuracy than the Word2Vec feature. Also, by comparing the two simple models, SVM tends to learning the features of texts better than the Perceptron. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import confusion_matrix\n",
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Average Word2Vec vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Reference: https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook <font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ls_avg=[]\n",
    "for i in range(len(my_dataset)):\n",
    "    vec=[]\n",
    "    for j in my_dataset[i]:\n",
    "        if j in model_g.key_to_index:\n",
    "            xxx=model_g[j].astype('float32')\n",
    "            vec.append(xxx)\n",
    "        \n",
    "    if not vec:\n",
    "        vec=[np.zeros(model_g[my_dataset[0][0]].shape, dtype='float32')]\n",
    "    a=np.mean(vec, dtype='float32', axis=0)\n",
    "    w2v_ls_avg.append(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=temp['star_rating'].values\n",
    "y=y.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3_train, X3_test, y3_train, y3_test = train_test_split(w2v_ls_avg, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, all the labels will minus by one since in cross-entropy, the targets always begin with 0.\n",
    "# Also, the training dataset will be split into validation and training\n",
    "len_train = int(0.8 * len(X3_train))\n",
    "X_train_avg, y_train_avg = X3_train[:len_train], []\n",
    "for i in range(len_train):\n",
    "    y_train_avg.append(y3_train[i] - 1) #all the labels minus by one\n",
    "\n",
    "len_valid = len(X3_train) - len_train\n",
    "X_valid_avg, y_valid_avg = X3_train[len_train:], []\n",
    "for i in range(len_valid):\n",
    "    y_valid_avg.append(y3_train[len_train + i] - 1) #all the labels minus by one\n",
    "\n",
    "X_test_avg, y_test_avg = X3_test, y3_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the label type from float64 to float32\n",
    "def changetype(x):\n",
    "    xx=np.array(x, dtype='float32')\n",
    "    b=xx.tolist()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_avg=changetype(y_train_avg)\n",
    "y_test_avg=changetype(y_test_avg)\n",
    "y_valid_avg=changetype(y_valid_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a customed dataset class which will be mainly used for dataloader\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, features, labels):        \n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Convert lists to tensors\n",
    "        X = torch.tensor(self.features[index])\n",
    "        y = self.labels[index]\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation and testing datasets\n",
    "train_set = Dataset(X_train_avg, y_train_avg)\n",
    "valid_set = Dataset(X_valid_avg, y_valid_avg)\n",
    "test_set = Dataset(X_test_avg, y_test_avg)\n",
    "\n",
    "# Generate dataloaders for the training, validation and testing datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, **params)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feedforward neural network model\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden1, hidden2):\n",
    "        super(FNN, self).__init__()\n",
    "        self.input_size=input_size\n",
    "        self.hidden1=hidden1\n",
    "        self.hidden2=hidden2\n",
    "        \n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(self.input_size, self.hidden1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(self.hidden1, self.hidden2)\n",
    "        # linear layer (n_hidden -> 5)\n",
    "        self.fc3 = nn.Linear(self.hidden2, 5)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the NN\n",
    "fnn = FNN(300, 50, 10)\n",
    "print(fnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.458558 \tValidation Loss: 1.238879\n",
      "Validation loss decreased (inf --> 1.238879).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.242959 \tValidation Loss: 1.158720\n",
      "Validation loss decreased (1.238879 --> 1.158720).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.196788 \tValidation Loss: 1.122065\n",
      "Validation loss decreased (1.158720 --> 1.122065).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.162667 \tValidation Loss: 1.091022\n",
      "Validation loss decreased (1.122065 --> 1.091022).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.143059 \tValidation Loss: 1.103347\n",
      "Epoch: 6 \tTraining Loss: 1.125532 \tValidation Loss: 1.061989\n",
      "Validation loss decreased (1.091022 --> 1.061989).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.109783 \tValidation Loss: 1.047499\n",
      "Validation loss decreased (1.061989 --> 1.047499).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.098899 \tValidation Loss: 1.050907\n",
      "Epoch: 9 \tTraining Loss: 1.088954 \tValidation Loss: 1.018150\n",
      "Validation loss decreased (1.047499 --> 1.018150).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.078668 \tValidation Loss: 1.056124\n",
      "Epoch: 11 \tTraining Loss: 1.072037 \tValidation Loss: 1.001310\n",
      "Validation loss decreased (1.018150 --> 1.001310).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.066735 \tValidation Loss: 0.995781\n",
      "Validation loss decreased (1.001310 --> 0.995781).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.060454 \tValidation Loss: 1.002148\n",
      "Epoch: 14 \tTraining Loss: 1.054554 \tValidation Loss: 0.992528\n",
      "Validation loss decreased (0.995781 --> 0.992528).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.048407 \tValidation Loss: 0.984884\n",
      "Validation loss decreased (0.992528 --> 0.984884).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.042075 \tValidation Loss: 0.984333\n",
      "Validation loss decreased (0.984884 --> 0.984333).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.040464 \tValidation Loss: 0.979903\n",
      "Validation loss decreased (0.984333 --> 0.979903).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.034140 \tValidation Loss: 0.981488\n",
      "Epoch: 19 \tTraining Loss: 1.028645 \tValidation Loss: 0.996062\n",
      "Epoch: 20 \tTraining Loss: 1.027433 \tValidation Loss: 0.966371\n",
      "Validation loss decreased (0.979903 --> 0.966371).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.022560 \tValidation Loss: 0.979704\n",
      "Epoch: 22 \tTraining Loss: 1.013939 \tValidation Loss: 0.963658\n",
      "Validation loss decreased (0.966371 --> 0.963658).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.013589 \tValidation Loss: 0.968107\n",
      "Epoch: 24 \tTraining Loss: 1.011947 \tValidation Loss: 0.955717\n",
      "Validation loss decreased (0.963658 --> 0.955717).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.010887 \tValidation Loss: 0.962011\n",
      "Epoch: 26 \tTraining Loss: 1.004180 \tValidation Loss: 0.969984\n",
      "Epoch: 27 \tTraining Loss: 1.005749 \tValidation Loss: 0.994749\n",
      "Epoch: 28 \tTraining Loss: 1.001557 \tValidation Loss: 0.966716\n",
      "Epoch: 29 \tTraining Loss: 0.999765 \tValidation Loss: 0.960493\n",
      "Epoch: 30 \tTraining Loss: 0.994204 \tValidation Loss: 0.958727\n",
      "Epoch: 31 \tTraining Loss: 0.997880 \tValidation Loss: 0.962569\n",
      "Epoch: 32 \tTraining Loss: 0.993864 \tValidation Loss: 0.954632\n",
      "Validation loss decreased (0.955717 --> 0.954632).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.992144 \tValidation Loss: 0.950387\n",
      "Validation loss decreased (0.954632 --> 0.950387).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.991804 \tValidation Loss: 0.949122\n",
      "Validation loss decreased (0.950387 --> 0.949122).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 0.990411 \tValidation Loss: 0.951855\n",
      "Epoch: 36 \tTraining Loss: 0.988119 \tValidation Loss: 0.962147\n",
      "Epoch: 37 \tTraining Loss: 0.984658 \tValidation Loss: 0.956231\n",
      "Epoch: 38 \tTraining Loss: 0.985086 \tValidation Loss: 0.953070\n",
      "Epoch: 39 \tTraining Loss: 0.982578 \tValidation Loss: 0.947877\n",
      "Validation loss decreased (0.949122 --> 0.947877).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 0.982161 \tValidation Loss: 0.947762\n",
      "Validation loss decreased (0.947877 --> 0.947762).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 0.977780 \tValidation Loss: 0.944030\n",
      "Validation loss decreased (0.947762 --> 0.944030).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 0.979262 \tValidation Loss: 0.955372\n",
      "Epoch: 43 \tTraining Loss: 0.978197 \tValidation Loss: 0.957980\n",
      "Epoch: 44 \tTraining Loss: 0.975806 \tValidation Loss: 0.951239\n",
      "Epoch: 45 \tTraining Loss: 0.974541 \tValidation Loss: 0.955304\n",
      "Epoch: 46 \tTraining Loss: 0.975512 \tValidation Loss: 0.943168\n",
      "Validation loss decreased (0.944030 --> 0.943168).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.976029 \tValidation Loss: 0.947860\n",
      "Epoch: 48 \tTraining Loss: 0.975407 \tValidation Loss: 0.951324\n",
      "Epoch: 49 \tTraining Loss: 0.968571 \tValidation Loss: 0.949734\n",
      "Epoch: 50 \tTraining Loss: 0.970591 \tValidation Loss: 0.949479\n",
      "Epoch: 51 \tTraining Loss: 0.967142 \tValidation Loss: 0.950827\n",
      "Epoch: 52 \tTraining Loss: 0.966046 \tValidation Loss: 0.948033\n",
      "Epoch: 53 \tTraining Loss: 0.965507 \tValidation Loss: 0.956799\n",
      "Epoch: 54 \tTraining Loss: 0.964501 \tValidation Loss: 0.944146\n",
      "Epoch: 55 \tTraining Loss: 0.964654 \tValidation Loss: 0.952589\n",
      "Epoch: 56 \tTraining Loss: 0.963251 \tValidation Loss: 0.948980\n",
      "Epoch: 57 \tTraining Loss: 0.959228 \tValidation Loss: 0.949407\n",
      "Epoch: 58 \tTraining Loss: 0.961722 \tValidation Loss: 0.945082\n",
      "Epoch: 59 \tTraining Loss: 0.961729 \tValidation Loss: 0.947363\n",
      "Epoch: 60 \tTraining Loss: 0.959522 \tValidation Loss: 0.947489\n",
      "Epoch: 61 \tTraining Loss: 0.957432 \tValidation Loss: 0.945756\n",
      "Epoch: 62 \tTraining Loss: 0.955251 \tValidation Loss: 0.948201\n",
      "Epoch: 63 \tTraining Loss: 0.954558 \tValidation Loss: 0.942328\n",
      "Validation loss decreased (0.943168 --> 0.942328).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.955011 \tValidation Loss: 0.952209\n",
      "Epoch: 65 \tTraining Loss: 0.956725 \tValidation Loss: 0.950874\n",
      "Epoch: 66 \tTraining Loss: 0.952517 \tValidation Loss: 0.945794\n",
      "Epoch: 67 \tTraining Loss: 0.953823 \tValidation Loss: 0.953088\n",
      "Epoch: 68 \tTraining Loss: 0.953000 \tValidation Loss: 0.946046\n",
      "Epoch: 69 \tTraining Loss: 0.951804 \tValidation Loss: 0.942127\n",
      "Validation loss decreased (0.942328 --> 0.942127).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.949718 \tValidation Loss: 0.943420\n",
      "Epoch: 71 \tTraining Loss: 0.949474 \tValidation Loss: 0.948007\n",
      "Epoch: 72 \tTraining Loss: 0.945521 \tValidation Loss: 0.949490\n",
      "Epoch: 73 \tTraining Loss: 0.946208 \tValidation Loss: 0.945985\n",
      "Epoch: 74 \tTraining Loss: 0.942945 \tValidation Loss: 0.949267\n",
      "Epoch: 75 \tTraining Loss: 0.943790 \tValidation Loss: 0.945287\n",
      "Epoch: 76 \tTraining Loss: 0.944242 \tValidation Loss: 0.956325\n",
      "Epoch: 77 \tTraining Loss: 0.941870 \tValidation Loss: 0.956123\n",
      "Epoch: 78 \tTraining Loss: 0.942539 \tValidation Loss: 0.950544\n",
      "Epoch: 79 \tTraining Loss: 0.941838 \tValidation Loss: 0.948308\n",
      "Epoch: 80 \tTraining Loss: 0.940326 \tValidation Loss: 0.943682\n",
      "Epoch: 81 \tTraining Loss: 0.939381 \tValidation Loss: 0.948852\n",
      "Epoch: 82 \tTraining Loss: 0.938616 \tValidation Loss: 0.955791\n",
      "Epoch: 83 \tTraining Loss: 0.937186 \tValidation Loss: 0.958793\n",
      "Epoch: 84 \tTraining Loss: 0.937528 \tValidation Loss: 0.944824\n",
      "Epoch: 85 \tTraining Loss: 0.936087 \tValidation Loss: 0.948224\n",
      "Epoch: 86 \tTraining Loss: 0.937148 \tValidation Loss: 0.948972\n",
      "Epoch: 87 \tTraining Loss: 0.935596 \tValidation Loss: 0.944441\n",
      "Epoch: 88 \tTraining Loss: 0.935523 \tValidation Loss: 0.989043\n",
      "Epoch: 89 \tTraining Loss: 0.933555 \tValidation Loss: 0.961533\n",
      "Epoch: 90 \tTraining Loss: 0.932017 \tValidation Loss: 0.961623\n",
      "Epoch: 91 \tTraining Loss: 0.933244 \tValidation Loss: 0.942825\n",
      "Epoch: 92 \tTraining Loss: 0.930725 \tValidation Loss: 0.945002\n",
      "Epoch: 93 \tTraining Loss: 0.929519 \tValidation Loss: 0.951214\n",
      "Epoch: 94 \tTraining Loss: 0.932398 \tValidation Loss: 0.945180\n",
      "Epoch: 95 \tTraining Loss: 0.931237 \tValidation Loss: 0.951777\n",
      "Epoch: 96 \tTraining Loss: 0.931136 \tValidation Loss: 0.945442\n",
      "Epoch: 97 \tTraining Loss: 0.931690 \tValidation Loss: 0.950856\n",
      "Epoch: 98 \tTraining Loss: 0.930111 \tValidation Loss: 0.947739\n",
      "Epoch: 99 \tTraining Loss: 0.929372 \tValidation Loss: 0.955732\n",
      "Epoch: 100 \tTraining Loss: 0.928486 \tValidation Loss: 0.961309\n",
      "Epoch: 101 \tTraining Loss: 0.927657 \tValidation Loss: 0.958030\n",
      "Epoch: 102 \tTraining Loss: 0.926762 \tValidation Loss: 0.947725\n",
      "Epoch: 103 \tTraining Loss: 0.926020 \tValidation Loss: 0.947604\n",
      "Epoch: 104 \tTraining Loss: 0.924746 \tValidation Loss: 0.966503\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 105 \tTraining Loss: 0.924154 \tValidation Loss: 0.948027\n",
      "Epoch: 106 \tTraining Loss: 0.925113 \tValidation Loss: 0.965709\n",
      "Epoch: 107 \tTraining Loss: 0.921420 \tValidation Loss: 0.958744\n",
      "Epoch: 108 \tTraining Loss: 0.924620 \tValidation Loss: 0.952713\n",
      "Epoch: 109 \tTraining Loss: 0.924152 \tValidation Loss: 0.950189\n",
      "Epoch: 110 \tTraining Loss: 0.924000 \tValidation Loss: 0.957738\n",
      "Epoch: 111 \tTraining Loss: 0.921112 \tValidation Loss: 0.946453\n",
      "Epoch: 112 \tTraining Loss: 0.924178 \tValidation Loss: 0.947657\n",
      "Epoch: 113 \tTraining Loss: 0.920981 \tValidation Loss: 0.956296\n",
      "Epoch: 114 \tTraining Loss: 0.918902 \tValidation Loss: 0.961494\n",
      "Epoch: 115 \tTraining Loss: 0.919674 \tValidation Loss: 0.950827\n",
      "Epoch: 116 \tTraining Loss: 0.919984 \tValidation Loss: 0.945964\n",
      "Epoch: 117 \tTraining Loss: 0.917030 \tValidation Loss: 0.954657\n",
      "Epoch: 118 \tTraining Loss: 0.920527 \tValidation Loss: 0.950149\n",
      "Epoch: 119 \tTraining Loss: 0.919222 \tValidation Loss: 0.957745\n",
      "Epoch: 120 \tTraining Loss: 0.918417 \tValidation Loss: 0.955087\n"
     ]
    }
   ],
   "source": [
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(fnn.parameters(), lr=0.15)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 120\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    fnn.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = fnn(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    fnn.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = fnn(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(fnn.state_dict(), 'fnn.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the test dataset with model\n",
    "def predict(model, dataloader):\n",
    "    y_pred = []\n",
    "    y_test = []\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        outputs = model(batch[0])\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.append(predicted)\n",
    "        y_test.append(batch[1])\n",
    "    return y_pred, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the predictations into a list\n",
    "def get_pred(pred_label, test_label):\n",
    "    pred=pred_label\n",
    "    label=test_label\n",
    "    pred = [list(torch.Tensor.numpy(t)) for t in pred]\n",
    "    label = [list(torch.Tensor.numpy(t)) for t in label]\n",
    "    # Convert predictions and truths from list of lists to a single list\n",
    "    pred = functools.reduce(lambda a, b: a + b, pred)\n",
    "    label = functools.reduce(lambda a, b: a + b, label)\n",
    "    # Convert predictions by adding all labels by one\n",
    "    pred = [p + 1 for p in pred]\n",
    "    \n",
    "    return pred, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model parameters from the trained model\n",
    "fnn.load_state_dict(torch.load('fnn.pt'))\n",
    "pred, label = predict(fnn, test_loader)\n",
    "# Convert predictions and truths from list of tensors to list of lists\n",
    "pred = [list(torch.Tensor.numpy(t)) for t in pred]\n",
    "label = [list(torch.Tensor.numpy(t)) for t in label]\n",
    "# Convert predictions and truths from list of lists to a single list\n",
    "pred = functools.reduce(lambda a, b: a + b, pred)\n",
    "label = functools.reduce(lambda a, b: a + b, label)\n",
    "# Convert predictions by adding all labels by one\n",
    "pred = [p + 1 for p in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing dataset:0.5869\n"
     ]
    }
   ],
   "source": [
    "fnn.load_state_dict(torch.load('fnn.pt'))\n",
    "pred, label = predict(fnn, test_loader)\n",
    "y_pred, y_true = get_pred(pred, label)\n",
    "# report accuracy\n",
    "print('Accuracy of testing dataset by FNN:'+ str(accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. First 10 W2V "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first ten Word2Vec feature extraction\n",
    "w2v_ls_ten=[]\n",
    "for i in range(len(my_dataset)):\n",
    "    vec=[]\n",
    "    ten_words=my_dataset[i][:10]\n",
    "    for j in ten_words:\n",
    "        if j in model_g.key_to_index:\n",
    "            xxx=model_g[j].astype('float32')\n",
    "            xxx=list(xxx)\n",
    "            vec.append(xxx)\n",
    "            \n",
    "    w2v_ls_ten.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that adds padding to the reviews with length less than 10 words\n",
    "def add_padding(x):\n",
    "    padding = [0 for _ in range(300)]\n",
    "    if len(x) < 10:\n",
    "        for i in range(10 - len(x)):\n",
    "            x.append(padding)\n",
    "    return x\n",
    "\n",
    "# add paddings\n",
    "train_ten = [add_padding(x) for x in w2v_ls_ten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ten = [functools.reduce(lambda a, b: a + b, x) for x in train_ten]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4_train, X4_test, y4_train, y4_test = train_test_split(train_ten, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the training dataset into 0.2 validation dataset and 0.8 training\n",
    "len_train = int(0.8 * len(X4_train))\n",
    "X_train_ten, y_train_ten = X4_train[:len_train], []\n",
    "for i in range(len_train):\n",
    "    y_train_ten.append(y4_train[i] - 1) #all labels minus one\n",
    "\n",
    "len_valid = len(X4_train) - len_train\n",
    "X_valid_ten, y_valid_ten = X4_train[len_train:], []\n",
    "for i in range(len_valid):\n",
    "    y_valid_ten.append(y4_train[len_train + i] - 1) #all labels minus one\n",
    "\n",
    "X_test_ten, y_test_ten = X4_test, y4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the dtpe into float32\n",
    "def changetype(x):\n",
    "    xx=np.array(x, dtype='float32')\n",
    "    b=xx.tolist()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_ten=changetype(y_train_ten)\n",
    "y_test_ten=changetype(y_test_ten)\n",
    "y_valid_ten=changetype(y_valid_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation and testing datasets\n",
    "train_set = Dataset(X_train_ten, y_train_ten)\n",
    "valid_set = Dataset(X_valid_ten, y_valid_ten)\n",
    "test_set = Dataset(X_test_ten, y_test_ten)\n",
    "\n",
    "# Generate dataloaders for the training, validation and testing datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, **params)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNN(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fnn2 = FNN(3000, 50, 10)\n",
    "print(fnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cross entropy loss as loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(fnn2.parameters(), lr=0.08)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.337230 \tValidation Loss: 1.105510\n",
      "Validation loss decreased (inf --> 1.105510).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.110383 \tValidation Loss: 1.043747\n",
      "Validation loss decreased (1.105510 --> 1.043747).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.043294 \tValidation Loss: 1.008590\n",
      "Validation loss decreased (1.043747 --> 1.008590).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.998328 \tValidation Loss: 0.998881\n",
      "Validation loss decreased (1.008590 --> 0.998881).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 0.960319 \tValidation Loss: 1.000180\n",
      "Epoch: 6 \tTraining Loss: 0.927776 \tValidation Loss: 0.997713\n",
      "Validation loss decreased (0.998881 --> 0.997713).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.899605 \tValidation Loss: 1.016796\n",
      "Epoch: 8 \tTraining Loss: 0.870537 \tValidation Loss: 1.034686\n",
      "Epoch: 9 \tTraining Loss: 0.845514 \tValidation Loss: 1.032366\n",
      "Epoch: 10 \tTraining Loss: 0.824275 \tValidation Loss: 1.056447\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "# number of epochs to train the model\n",
    "n_epochs = 10\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    fnn2.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = fnn2(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    fnn2.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        output = fnn2(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(fnn2.state_dict(), 'fnn2.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing dataset by FNN:0.57085\n"
     ]
    }
   ],
   "source": [
    "# Load model parameters from the trained model\n",
    "fnn2.load_state_dict(torch.load('fnn2.pt'))\n",
    "pred, label = predict(fnn2, test_loader)\n",
    "y_pred2, y_true2 = get_pred(pred, label)\n",
    "# report accuracy, precision, recall, and f1-score on the testing dataset\n",
    "print('Accuracy of testing dataset by FNN:' + str(accuracy_score(y_true2, y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Explanation: \n",
    " Firstly, in Word2Vec cases, the Feedforward Neural Network performs better than the Perceptron model. In case of 'Average Word2Vec', the FNN results a better accuracy than the Perceptron one. In case of 'First ten Word2Vec', the FNN model reach a higher accuracy than the Perceptron one.  However, SVM model still betters two neural network models.\n",
    "    \n",
    "    \n",
    "Secondly, TF-IDF features has a better accuracy than the Word2Vec features though different models are applied.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Simple RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_ls_twty=[]\n",
    "for i in range(len(my_dataset)):\n",
    "    #x=np.zeros(model[my_dataset[0][0]].shape)\n",
    "    vec=[]\n",
    "    twty_words=my_dataset[i][:20]\n",
    "    for j in twty_words:\n",
    "        if j in model_g.key_to_index:\n",
    "            xxx=model_g[j].astype('float32')\n",
    "            xxx=list(xxx)\n",
    "            vec.append(xxx)\n",
    "            \n",
    "    w2v_ls_twty.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(x):\n",
    "    padding = [0 for _ in range(300)]\n",
    "    if len(x) < 20:\n",
    "        for i in range(20 - len(x)):\n",
    "            x.append(padding)\n",
    "    return x\n",
    "\n",
    "# add paddings\n",
    "train_twty = [add_padding(x) for x in w2v_ls_twty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., ..., 5., 5., 5.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=temp['star_rating'].values\n",
    "y=y.astype('float32')\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5_train, X5_test, y5_train, y5_test = train_test_split(train_twty, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train = int(0.8 * len(X5_train))\n",
    "X_train_twty, y_train_twty = X5_train[:len_train], []\n",
    "for i in range(len_train):\n",
    "    y_train_twty.append(y5_train[i] - 1) # Convert all labels by minus one\n",
    "\n",
    "len_valid = len(X5_train) - len_train\n",
    "X_valid_twty, y_valid_twty = X5_train[len_train:], []\n",
    "for i in range(len_valid):\n",
    "    y_valid_twty.append(y5_train[len_train + i] - 1)\n",
    "\n",
    "X_test_twty, y_test_twty = X5_test, y5_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_twty=changetype(y_train_twty)\n",
    "y_test_twty=changetype(y_test_twty)\n",
    "y_valid_twty=changetype(y_valid_twty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, validation and testing datasets\n",
    "train_set = Dataset(X_train_twty, y_train_twty)\n",
    "valid_set = Dataset(X_valid_twty, y_valid_twty)\n",
    "test_set = Dataset(X_test_twty, y_test_twty)\n",
    "\n",
    "# Generate dataloaders for the training, validation and testing datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_set, **params)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, **params)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size=300, hidden_size=20, output_size=5):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size \n",
    "        self.output = output_size\n",
    "        self.hidden1 = hidden_size\n",
    "        \n",
    "        # linear layer (input --> hidden)\n",
    "        self.i2h = nn.Linear(self.input_size + self.hidden1, self.hidden1)\n",
    "        # linear layer (hidden --> output)\n",
    "        self.i2o = nn.Linear(self.input_size + self.hidden1, self.output)\n",
    "    \n",
    "    def forward(self, wordVec):\n",
    "        batch_size = wordVec.shape[0]\n",
    "        hidden = torch.zeros(batch_size, self.hidden1)\n",
    "        combined = torch.cat((wordVec[:,0,], hidden), 1)\n",
    "        for i in range(1, 20):\n",
    "            hidden = self.i2h(combined)\n",
    "            combined = torch.cat((wordVec[:,i,], hidden), 1)\n",
    "        output = self.i2o(combined)\n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (i2h): Linear(in_features=320, out_features=20, bias=True)\n",
      "  (i2o): Linear(in_features=320, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cross entropy loss as loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(rnn.parameters(), lr=0.008)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.609447 \tValidation Loss: 1.608321\n",
      "Validation loss decreased (inf --> 1.608321).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.607923 \tValidation Loss: 1.607267\n",
      "Validation loss decreased (1.608321 --> 1.607267).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.606779 \tValidation Loss: 1.606428\n",
      "Validation loss decreased (1.607267 --> 1.606428).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.605896 \tValidation Loss: 1.605782\n",
      "Validation loss decreased (1.606428 --> 1.605782).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.605138 \tValidation Loss: 1.605282\n",
      "Validation loss decreased (1.605782 --> 1.605282).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.604504 \tValidation Loss: 1.604854\n",
      "Validation loss decreased (1.605282 --> 1.604854).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.603940 \tValidation Loss: 1.604416\n",
      "Validation loss decreased (1.604854 --> 1.604416).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.603428 \tValidation Loss: 1.604082\n",
      "Validation loss decreased (1.604416 --> 1.604082).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.602955 \tValidation Loss: 1.603743\n",
      "Validation loss decreased (1.604082 --> 1.603743).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.602507 \tValidation Loss: 1.603487\n",
      "Validation loss decreased (1.603743 --> 1.603487).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.602096 \tValidation Loss: 1.603191\n",
      "Validation loss decreased (1.603487 --> 1.603191).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.601692 \tValidation Loss: 1.602976\n",
      "Validation loss decreased (1.603191 --> 1.602976).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.601308 \tValidation Loss: 1.602677\n",
      "Validation loss decreased (1.602976 --> 1.602677).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.600932 \tValidation Loss: 1.602465\n",
      "Validation loss decreased (1.602677 --> 1.602465).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.600554 \tValidation Loss: 1.602221\n",
      "Validation loss decreased (1.602465 --> 1.602221).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.600179 \tValidation Loss: 1.601938\n",
      "Validation loss decreased (1.602221 --> 1.601938).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.599813 \tValidation Loss: 1.601725\n",
      "Validation loss decreased (1.601938 --> 1.601725).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.599436 \tValidation Loss: 1.601527\n",
      "Validation loss decreased (1.601725 --> 1.601527).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.599046 \tValidation Loss: 1.601256\n",
      "Validation loss decreased (1.601527 --> 1.601256).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.598633 \tValidation Loss: 1.600919\n",
      "Validation loss decreased (1.601256 --> 1.600919).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.598216 \tValidation Loss: 1.600673\n",
      "Validation loss decreased (1.600919 --> 1.600673).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.597760 \tValidation Loss: 1.600306\n",
      "Validation loss decreased (1.600673 --> 1.600306).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.597275 \tValidation Loss: 1.599933\n",
      "Validation loss decreased (1.600306 --> 1.599933).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.596734 \tValidation Loss: 1.599500\n",
      "Validation loss decreased (1.599933 --> 1.599500).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.596127 \tValidation Loss: 1.599027\n",
      "Validation loss decreased (1.599500 --> 1.599027).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.595428 \tValidation Loss: 1.598439\n",
      "Validation loss decreased (1.599027 --> 1.598439).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.594591 \tValidation Loss: 1.597639\n",
      "Validation loss decreased (1.598439 --> 1.597639).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.593511 \tValidation Loss: 1.596629\n",
      "Validation loss decreased (1.597639 --> 1.596629).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.592037 \tValidation Loss: 1.595002\n",
      "Validation loss decreased (1.596629 --> 1.595002).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.589333 \tValidation Loss: 1.591114\n",
      "Validation loss decreased (1.595002 --> 1.591114).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.482210 \tValidation Loss: 1.296426\n",
      "Validation loss decreased (1.591114 --> 1.296426).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.277134 \tValidation Loss: 1.246633\n",
      "Validation loss decreased (1.296426 --> 1.246633).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.254935 \tValidation Loss: 1.242661\n",
      "Validation loss decreased (1.246633 --> 1.242661).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.243934 \tValidation Loss: 1.247850\n",
      "Epoch: 35 \tTraining Loss: 1.237377 \tValidation Loss: 1.231327\n",
      "Validation loss decreased (1.242661 --> 1.231327).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.234545 \tValidation Loss: 1.229344\n",
      "Validation loss decreased (1.231327 --> 1.229344).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.232535 \tValidation Loss: 1.259948\n",
      "Epoch: 38 \tTraining Loss: 1.230103 \tValidation Loss: 1.220228\n",
      "Validation loss decreased (1.229344 --> 1.220228).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.228840 \tValidation Loss: 1.218694\n",
      "Validation loss decreased (1.220228 --> 1.218694).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.225897 \tValidation Loss: 1.224129\n",
      "Epoch: 41 \tTraining Loss: 1.225667 \tValidation Loss: 1.248208\n",
      "Epoch: 42 \tTraining Loss: 1.225167 \tValidation Loss: 1.235066\n",
      "Epoch: 43 \tTraining Loss: 1.224031 \tValidation Loss: 1.208516\n",
      "Validation loss decreased (1.218694 --> 1.208516).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.207404 \tValidation Loss: 1.188987\n",
      "Validation loss decreased (1.208516 --> 1.188987).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.181292 \tValidation Loss: 1.168640\n",
      "Validation loss decreased (1.188987 --> 1.168640).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.161400 \tValidation Loss: 1.171154\n",
      "Epoch: 47 \tTraining Loss: 1.150659 \tValidation Loss: 1.142586\n",
      "Validation loss decreased (1.168640 --> 1.142586).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.143818 \tValidation Loss: 1.142397\n",
      "Validation loss decreased (1.142586 --> 1.142397).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.139270 \tValidation Loss: 1.122382\n",
      "Validation loss decreased (1.142397 --> 1.122382).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.136710 \tValidation Loss: 1.152063\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    rnn.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        # transfer to GPU\n",
    "        #data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "\n",
    "        output = rnn(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    rnn.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        output = rnn(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(rnn.state_dict(), 'rnn.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing dataset by RNN:0.52465\n"
     ]
    }
   ],
   "source": [
    "# Load model parameters from the trained model \n",
    "rnn.load_state_dict(torch.load('rnn.pt'))\n",
    "pred, label = predict(rnn, test_loader)\n",
    "y_pred3, y_true3 = get_pred(pred, label)\n",
    "# report accuracy, precision, recall, and f1-score on the testing dataset\n",
    "print('Accuracy of testing dataset by RNN:' + str(accuracy_score(y_true3, y_pred3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Explanation: In case of 'first N Word2Vec', FNN performs better than RNN with respect to the accuracy. Since the FNN only take the first 10 word which in simple analysis, it is more likely to contain less feature representations. Therefore, a conclusion can be drawn prematurely that FNN did better in first N Word2Vec cases.\n",
    "\n",
    "Moreover, in case of 'averge Word2Vec', the RNN in this case still does not perform better than the FNN one.\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gated Recurrent Unit Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gated Recurrent Unit cell\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # linear layer for reset gate\n",
    "        self.fc1 = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "        # linear layer for update gate\n",
    "        self.fc2 = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "        # linear layer for intermediate hidden state\n",
    "        self.fc3 = nn.Linear(self.input_size + self.hidden_size, self.hidden_size)\n",
    "        # linear layer for output\n",
    "        self.fc4 = nn.Linear(self.input_size + self.hidden_size, self.output_size)\n",
    "    \n",
    "    def forward(self, wordVec):\n",
    "       \n",
    "        batch_size = wordVec.shape[0]\n",
    "        hidden = torch.zeros(batch_size, self.hidden_size)\n",
    "        combined = torch.cat((wordVec[:,0,], hidden), 1)\n",
    "        sigmoid, tanh = nn.Sigmoid(), nn.Tanh()\n",
    "        for i in range(1, 20):\n",
    "            # reset gate\n",
    "            reset = self.fc1(combined)\n",
    "            reset = sigmoid(reset)\n",
    "            # update gate\n",
    "            update = self.fc2(combined)\n",
    "            update = sigmoid(update)\n",
    "            # previous hidden state passes through reset gate\n",
    "            hidden2 = torch.mul(reset, hidden)\n",
    "            # intermediate hidden state\n",
    "            combined2 = torch.cat((wordVec[:,0,], hidden2), 1)\n",
    "            hidden_inter = self.fc3(combined2)\n",
    "            hidden_inter = tanh(hidden_inter)\n",
    "            # hidden state\n",
    "            hidden = torch.mul(1-update, hidden) + torch.mul(update, hidden_inter)\n",
    "            combined = torch.cat((wordVec[:,i,], hidden), 1)\n",
    "        output = self.fc4(combined)\n",
    "       \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (fc1): Linear(in_features=320, out_features=20, bias=True)\n",
      "  (fc2): Linear(in_features=320, out_features=20, bias=True)\n",
      "  (fc3): Linear(in_features=320, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=320, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# initialize the GRU\n",
    "gru = GRU(300, 20, 5)\n",
    "print(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify cross entropy loss as loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer = torch.optim.SGD(gru.parameters(), lr=0.005)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.608309 \tValidation Loss: 1.604893\n",
      "Validation loss decreased (inf --> 1.604893).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.601456 \tValidation Loss: 1.595862\n",
      "Validation loss decreased (1.604893 --> 1.595862).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.586856 \tValidation Loss: 1.573489\n",
      "Validation loss decreased (1.595862 --> 1.573489).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.552391 \tValidation Loss: 1.524219\n",
      "Validation loss decreased (1.573489 --> 1.524219).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.490596 \tValidation Loss: 1.452898\n",
      "Validation loss decreased (1.524219 --> 1.452898).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.424194 \tValidation Loss: 1.395523\n",
      "Validation loss decreased (1.452898 --> 1.395523).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.378603 \tValidation Loss: 1.358755\n",
      "Validation loss decreased (1.395523 --> 1.358755).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.347703 \tValidation Loss: 1.331320\n",
      "Validation loss decreased (1.358755 --> 1.331320).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.323315 \tValidation Loss: 1.308795\n",
      "Validation loss decreased (1.331320 --> 1.308795).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.302933 \tValidation Loss: 1.289726\n",
      "Validation loss decreased (1.308795 --> 1.289726).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.285483 \tValidation Loss: 1.273511\n",
      "Validation loss decreased (1.289726 --> 1.273511).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.270606 \tValidation Loss: 1.260197\n",
      "Validation loss decreased (1.273511 --> 1.260197).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.258309 \tValidation Loss: 1.249327\n",
      "Validation loss decreased (1.260197 --> 1.249327).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.247949 \tValidation Loss: 1.240495\n",
      "Validation loss decreased (1.249327 --> 1.240495).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.239312 \tValidation Loss: 1.232777\n",
      "Validation loss decreased (1.240495 --> 1.232777).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.231755 \tValidation Loss: 1.226312\n",
      "Validation loss decreased (1.232777 --> 1.226312).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.224962 \tValidation Loss: 1.220022\n",
      "Validation loss decreased (1.226312 --> 1.220022).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.218582 \tValidation Loss: 1.214056\n",
      "Validation loss decreased (1.220022 --> 1.214056).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.212350 \tValidation Loss: 1.208852\n",
      "Validation loss decreased (1.214056 --> 1.208852).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.206356 \tValidation Loss: 1.202955\n",
      "Validation loss decreased (1.208852 --> 1.202955).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    gru.train() # prep model for training\n",
    "    for data, target in train_loader:\n",
    "        \n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    gru.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader:\n",
    "        output = gru(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.long())\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(gru.state_dict(), 'gru.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of testing dataset by GRU:0.52135\n"
     ]
    }
   ],
   "source": [
    "# Load model parameters from the trained model\n",
    "gru.load_state_dict(torch.load('gru.pt'))\n",
    "pred, label = predict(gru, test_loader)\n",
    "y_pred4, y_true4 = get_pred(pred, label)\n",
    "# report accuracy, precision, recall, and f1-score on the testing dataset\n",
    "print('Accuracy of testing dataset by GRU:' + str(accuracy_score(y_true4, y_pred4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=4> Explanation: In the same case of 'first 20 Word2Vec', RNN performs slightly better than GRU with respect to the accuracy. However, in some time, GRU also performs better. Thus, a conclusion can be drawn that two models performs equally well with first 20 Word2Vec features.  </font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
